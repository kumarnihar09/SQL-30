{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c99e148-7749-42da-8e8b-1b2a6b5e91b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd36980c-1403-42ea-b260-fdacd667ba24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"AccountsDF\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (3, 108939),\n",
    "    (2, 12747),\n",
    "    (8, 87709),\n",
    "    (6, 91796)\n",
    "]\n",
    "\n",
    "# Column names\n",
    "columns = [\"account_id\", \"income\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a2671a-473f-46b6-a4d7-6885065c6e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with cte as(\n",
    "select 'Low Salary' as category,\n",
    "sum(case when income <20000 then 1 else 0 end) as accounts_count\n",
    "from accounts\n",
    "union all\n",
    "select 'Average Salary' as category,\n",
    "sum(case when income between 20000 and 50000 then 1 else 0 end) as accounts_count\n",
    "from accounts\n",
    "union all\n",
    "select 'High Salary' as category,\n",
    "sum(case when income >50000 then 1 else 0 end) as accounts_count\n",
    "from accounts\n",
    ")\n",
    "select * from cte\n",
    "order by accounts_count desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8c3df9-51c7-4e13-9004-327e6d0a721a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "accounts_df = df.withColumn('category', when(col('income')<20000, 'Low Salary') \\\n",
    "    .when((col('income')>=20000) & (col('income')<=50000), 'Average Salary')    \\\n",
    "        .when(col('income')>50000, 'High Salary')\n",
    "        )\n",
    "\n",
    "category_counts_df = accounts_df.groupBy('category').agg(count('*').alias('accounts_count'))\n",
    "\n",
    "all_categories = spark.createDataFrame([\n",
    "    (\"Low Salary\",),\n",
    "    (\"Average Salary\",),\n",
    "    (\"High Salary\",)\n",
    "], [\"category\"])\n",
    "result_df = all_categories.join(category_counts_df, on='category', how='left').fillna(0)\n",
    "result_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1907. Count Salary Categories",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
